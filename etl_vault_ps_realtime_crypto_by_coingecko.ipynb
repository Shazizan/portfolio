{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOEGAJxriWVfVqATLIo86uF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shazizan/portfolio/blob/master/etl_vault_ps_realtime_crypto_by_coingecko.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Real-Time Crypto (CoinGecko) ETL with PySpark and GitHub**"
      ],
      "metadata": {
        "id": "syFhd-w9SqW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup & Data Preparation**"
      ],
      "metadata": {
        "id": "Etn4VfSgUZjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1 - Setup Configuration: Install Java & PySpark**"
      ],
      "metadata": {
        "id": "k4vsTiw3Te_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- !apt-get → runs a Bash command in Colab (like installing software).\n",
        "- !pip install → installs Python packages in Colab."
      ],
      "metadata": {
        "id": "Kcz1AyuiToWf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iExMMQrySnqJ",
        "outputId": "52808353-47c1-4e51-91fc-03d9a30c582a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jre\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-11-demo openjdk-11-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jdk\n",
            "  openjdk-11-jre x11-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 5,367 kB of archives.\n",
            "After this operation, 15.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.28+6-1ubuntu1~22.04.1 [214 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.28+6-1ubuntu1~22.04.1 [1,342 kB]\n",
            "Fetched 5,367 kB in 1s (5,392 kB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126666 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../1-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../2-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../3-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../4-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../5-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../6-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../7-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../8-openjdk-11-jre_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../9-openjdk-11-jdk_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Install Java 11 (required by Spark)\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "\n",
        "# Install PySpark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2 - Import PySpark & Create Spark Session**"
      ],
      "metadata": {
        "id": "x7w50X7PUGtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- SparkSession is like the “engine” that lets us run PySpark commands.\n",
        "- If we see Spark is ready!, we’re all set."
      ],
      "metadata": {
        "id": "byRKfyqOUSPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"CryptoETL\").getOrCreate()\n",
        "\n",
        "print(\"Spark is ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laMdB_3sUyjB",
        "outputId": "1dd6204a-5a1b-44e3-d1fb-b585afb0246b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract**"
      ],
      "metadata": {
        "id": "ktMz9_DPW5S6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extract data using Python (requests library)**"
      ],
      "metadata": {
        "id": "TVdRqdmaXCec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- PySpark doesn’t call APIs directly — we usually use Python to fetch the data first, then convert it into a Spark DataFrame."
      ],
      "metadata": {
        "id": "P2cGkDA9YRs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ Import required modules"
      ],
      "metadata": {
        "id": "7UDKwtDZYk7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "TGOhqTidYo1y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2️⃣ Fetch data from CoinGecko API (the output is in json format)"
      ],
      "metadata": {
        "id": "VHU_qeaXYwVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://api.coingecko.com/api/v3/simple/price?ids=bitcoin,ethereum&vs_currencies=usd\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if request was successful\n",
        "if response.status_code == 200:\n",
        "    print(\"Data fetched successfully!\")\n",
        "    data = response.json()\n",
        "    print(data)\n",
        "else:\n",
        "    print(\"Error fetching data:\", response.status_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa2uf4MTY0ZB",
        "outputId": "7e07f962-9469-47e0-e61d-b372a1975394"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fetched successfully!\n",
            "{'bitcoin': {'usd': 114546}, 'ethereum': {'usd': 4141.59}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformation**"
      ],
      "metadata": {
        "id": "gL6-5sIceOLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transform / convert JSON into PySpark-friendly format**"
      ],
      "metadata": {
        "id": "I2k1rMmVefQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- PySpark likes list of dictionaries. So we convert JSON into it."
      ],
      "metadata": {
        "id": "_xFiixCafE0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert JSON to list of dicts\n",
        "data_list = [{\"coin\": k, \"usd\": v[\"usd\"]} for k, v in data.items()]"
      ],
      "metadata": {
        "id": "HZmgRQYAe09K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kYbbe0fe_tz",
        "outputId": "56922871-83f3-41dd-cf19-7529a3b6a787"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'coin': 'bitcoin', 'usd': 114546}, {'coin': 'ethereum', 'usd': 4141.59}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create PySpark DataFrame**"
      ],
      "metadata": {
        "id": "1Pc_j_f-fbUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark session if not already\n",
        "#spark = SparkSession.builder.appName(\"CryptoETL\").getOrCreate() -- since I already created above.\n",
        "\n",
        "# Convert list of dicts to Spark DataFrame\n",
        "# df = spark.createDataFrame(data_list) -- use this one only for thr below error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "jYc3q3izf9Qc",
        "outputId": "be260257-03f8-42ac-d4b5-45da641d0f85"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[CANNOT_MERGE_TYPE] Can not merge type `LongType` and `DoubleType`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3781283811.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert list of dicts to Spark DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             )\n\u001b[0;32m-> 1443\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1444\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1486\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0mtupled_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0minfer_array_from_first_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacyInferArrayTypeFromFirstElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         schema = reduce(\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m             (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         fields = [\n\u001b[1;32m   1788\u001b[0m             StructField(\n\u001b[0;32m-> 1789\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_merge_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNullType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m             )\n\u001b[1;32m   1791\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m   1780\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CANNOT_MERGE_TYPE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m             \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data_type1\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_type2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_MERGE_TYPE] Can not merge type `LongType` and `DoubleType`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "This error is common in PySpark when we try to create a DF from a list of dictionaries but the values in the same column have mixed types — for example:\n",
        "\n",
        "- Some usd values are integers (LongType)\n",
        "- Some usd values are floats (DoubleType)\n",
        "\n",
        "As a result, PySpark cannot automatically merge LongType and DoubleType, so it throws CANNOT_MERGE_TYPE."
      ],
      "metadata": {
        "id": "H2ZtUjV8neQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fixing Error on Transformation / Convertion Part**"
      ],
      "metadata": {
        "id": "75altAKmpCsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Fixing error in the transformation / convertion part by ensuring all the usd values are in float.\n",
        "- then, repeat the step of creating spark dataframe"
      ],
      "metadata": {
        "id": "nhkumLuVqyZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all usd values are float\n",
        "data_list = [{\"coin\": k, \"usd\": float(v[\"usd\"])} for k, v in data.items()]\n",
        "\n",
        "# Create PySpark DataFrame\n",
        "df = spark.createDataFrame(data_list)"
      ],
      "metadata": {
        "id": "-bPuqCYAocf8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gbetf-ypiKE",
        "outputId": "d7b1eaea-220d-4824-f22c-690a62410729"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+\n",
            "|    coin|     usd|\n",
            "+--------+--------+\n",
            "| bitcoin|114546.0|\n",
            "|ethereum| 4141.59|\n",
            "+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Optional - Add Timestamp Column**"
      ],
      "metadata": {
        "id": "c0ZKS4R5n9QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp\n",
        "\n",
        "df = df.withColumn(\"timestamp\", current_timestamp())"
      ],
      "metadata": {
        "id": "HCYzgXnArlpX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X0wf4p8r2MY",
        "outputId": "cf15f763-15ba-4cfd-867d-b8cbd08c8392"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+--------------------+\n",
            "|    coin|     usd|           timestamp|\n",
            "+--------+--------+--------------------+\n",
            "| bitcoin|114546.0|2025-10-01 07:32:...|\n",
            "|ethereum| 4141.59|2025-10-01 07:32:...|\n",
            "+--------+--------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load**"
      ],
      "metadata": {
        "id": "kXG43-RVst46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I'm going to save the work as a JSON file and push it to my GitHub repository (pipelines-vault)"
      ],
      "metadata": {
        "id": "oBEvGS8fs20D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1 - Save PySpark DataFrame as JSON**"
      ],
      "metadata": {
        "id": "TeyW72OYtWsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save DataFrame as JSON locally in Colab\n",
        "df.coalesce(1).write.mode(\"overwrite\").json(\"crypto_prices\")"
      ],
      "metadata": {
        "id": "9fWnKEYDtbsQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- This creates a folder called crypto_prices in Colab.\n",
        "- Inside it, there will be a file like part-00000-xxxx.json containing my data.\n",
        "- .coalesce(1) - PySpark splits DataFrames across multiple partitions internally to process data in parallel.\n",
        "\n",
        "- When writing a DataFrame to a file, PySpark writes one file per partition by default & .coalesce(1) tells PySpark: Merge all partitions into a single partition before writing."
      ],
      "metadata": {
        "id": "l9FNCE01tqo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2 - Rename JSON file for GitHub upload**"
      ],
      "metadata": {
        "id": "53g_JngJxQPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#These are standard Python modules — no extra installation needed.\n",
        "import os       #used for interacting with the operating system (like folders, paths).\n",
        "import glob    #finds files/folders matching a pattern.\n",
        "import shutil  #used for moving or copying files.\n",
        "\n",
        "# Find the JSON file inside the folder\n",
        "json_file = glob.glob(\"crypto_prices/part-*.json\")[0]\n",
        "# Rename/move it to a simple file\n",
        "shutil.move(json_file, \"crypto_prices.json\")\n",
        "print(\"Saved as crypto_prices.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSlsnMVQvmvT",
        "outputId": "d464f8d2-ca6e-4685-c6c8-ba4ac1c6f48d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved as crypto_prices.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- glob.glob(pattern) → returns a list of file paths matching the pattern.\n",
        "- Pattern \"crypto_prices/part-*.json\" means:\n",
        "* (1) Go inside the folder crypto_prices\n",
        "* (2) Look for any file starting with part- and ending with .json\n",
        "\n",
        "Why needed?:\n",
        "- When PySpark writes JSON, it creates a file like part-00000-xxxx.json inside the folder.\n",
        "- We need to find this automatically because the filename has a random suffix.\n",
        "\n",
        "another insight:\n",
        "- glob.glob returns a list of matching files.\n",
        "- [0] takes the first file from the list.\n",
        "- In our case, there should only be one JSON file because we used .coalesce(1)."
      ],
      "metadata": {
        "id": "hsd7N1SWwQhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ After run the code above, I should have a single JSON file ready to push."
      ],
      "metadata": {
        "id": "y2DNY5kcxbaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3 - Upload JSON file to GitHub using API**"
      ],
      "metadata": {
        "id": "KdUDfBdbxrG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirement:\n",
        "- GitHub username\n",
        "- Repository name\n",
        "- Personal Access Token (PAT) with repo permission"
      ],
      "metadata": {
        "id": "qPqRpFfvx1g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "# Read JSON file\n",
        "with open(\"crypto_prices.json\", \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Encode content to base64 (GitHub API requirement)\n",
        "encoded_content = base64.b64encode(content.encode()).decode()\n",
        "\n",
        "# GitHub info\n",
        "username = \"Shazizan\"\n",
        "repo = \"pipeline-vault\"\n",
        "path = \"crypto_prices.json\"  # destination path in repo\n",
        "token = \"REPLACE_WITH_YPUR_OWN_TOKEN\"\n",
        "\n",
        "url = f\"https://api.github.com/repos/Shazizan/pipeline-vault/contents/crypto_prices.json\"\n",
        "\n",
        "# Upload JSON to GitHub\n",
        "response = requests.put(\n",
        "    url,\n",
        "    headers={\"Authorization\": f\"token {token}\"},\n",
        "    json={\n",
        "        \"message\": \"Add latest crypto prices\",\n",
        "        \"content\": encoded_content\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTARRGTLyAij",
        "outputId": "73470e09-aad7-4328-e51a-55b79bba5cbf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'content': {'name': 'crypto_prices.json', 'path': 'crypto_prices.json', 'sha': '05ce7cd2f7d348a019de99bf9a580bc250c9cc33', 'size': 146, 'url': 'https://api.github.com/repos/Shazizan/pipeline-vault/contents/crypto_prices.json?ref=main', 'html_url': 'https://github.com/Shazizan/pipeline-vault/blob/main/crypto_prices.json', 'git_url': 'https://api.github.com/repos/Shazizan/pipeline-vault/git/blobs/05ce7cd2f7d348a019de99bf9a580bc250c9cc33', 'download_url': 'https://raw.githubusercontent.com/Shazizan/pipeline-vault/main/crypto_prices.json', 'type': 'file', '_links': {'self': 'https://api.github.com/repos/Shazizan/pipeline-vault/contents/crypto_prices.json?ref=main', 'git': 'https://api.github.com/repos/Shazizan/pipeline-vault/git/blobs/05ce7cd2f7d348a019de99bf9a580bc250c9cc33', 'html': 'https://github.com/Shazizan/pipeline-vault/blob/main/crypto_prices.json'}}, 'commit': {'sha': 'c01139e40fe9eece77a96ac9129583ea7e6b8fe4', 'node_id': 'C_kwDOPznHZNoAKGMwMTEzOWU0MGZlOWVlY2U3N2E5NmFjOTEyOTU4M2VhN2U2YjhmZTQ', 'url': 'https://api.github.com/repos/Shazizan/pipeline-vault/git/commits/c01139e40fe9eece77a96ac9129583ea7e6b8fe4', 'html_url': 'https://github.com/Shazizan/pipeline-vault/commit/c01139e40fe9eece77a96ac9129583ea7e6b8fe4', 'author': {'name': 'zizan', 'email': '77826862+Shazizan@users.noreply.github.com', 'date': '2025-10-01T09:53:23Z'}, 'committer': {'name': 'zizan', 'email': '77826862+Shazizan@users.noreply.github.com', 'date': '2025-10-01T09:53:23Z'}, 'tree': {'sha': '689b7095f05f9b7e052eda3e36d1258650ff8b5a', 'url': 'https://api.github.com/repos/Shazizan/pipeline-vault/git/trees/689b7095f05f9b7e052eda3e36d1258650ff8b5a'}, 'message': 'Add latest crypto prices', 'parents': [{'sha': '71ecbf2ccbb1d529f6c6d8c23470e4d19be02fe8', 'url': 'https://api.github.com/repos/Shazizan/pipeline-vault/git/commits/71ecbf2ccbb1d529f6c6d8c23470e4d19be02fe8', 'html_url': 'https://github.com/Shazizan/pipeline-vault/commit/71ecbf2ccbb1d529f6c6d8c23470e4d19be02fe8'}], 'verification': {'verified': False, 'reason': 'unsigned', 'signature': None, 'payload': None, 'verified_at': None}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4 - Validate / Check GitHub**"
      ],
      "metadata": {
        "id": "HFzxpef0MoQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Go to repository (pipeline-vault) → the crypto_prices.json file with the real-time data should be available the\n",
        "- In principle, this notebook can be executed at any time to retrieve the latest prices and update the data on GitHub. This marks my first independent, hands-on experiment in managing streaming data."
      ],
      "metadata": {
        "id": "Dmu5t8L8M23p"
      }
    }
  ]
}