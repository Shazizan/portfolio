{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3SVNQcHeC2pVyC4Hq6jwa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shazizan/portfolio/blob/master/etl_vault_lambda_distribution_centers_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Method 3 of Using Lambda Fx**"
      ],
      "metadata": {
        "id": "odqLu2Is-AZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 1 - Import Required Libraries**"
      ],
      "metadata": {
        "id": "PBgCRqlq53Xx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yVrxlmY-4Q7X"
      },
      "outputs": [],
      "source": [
        "import requests               # make HTTP requests to GITHUB API\n",
        "import json                   # to work with JSON data format\n",
        "import csv                    # to parse CSV files\n",
        "import base64                 # GitHub API requires file content to be base64 encoded\n",
        "from io import StringIO       # allow to treat strings as file object"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 2 - Configuration**"
      ],
      "metadata": {
        "id": "WGHgmuW26GSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GitHub Personal Access Token - needed for authentication\n",
        "GITHUB_TOKEN = \"PLACE_YOUR_TOKEN_HERE\"\n",
        "\n",
        "# Source Repository Configuration (where CSV file is located?)\n",
        "SOURCE_OWNER = \"Shazizan\"                           # GitHub username/org of source repo\n",
        "SOURCE_REPO = \"data\"                                # Name of source repository\n",
        "SOURCE_FILE_PATH = \"distribution_centers.csv\"       # Path to CSV file in source repo\n",
        "\n",
        "# Destination Repository Configuration (where JSON file will be uploaded?)\n",
        "DEST_OWNER = \"Shazizan\"                              # GitHub username/org of destination repo\n",
        "DEST_REPO = \"pipeline-vault\"                         # Name of destination repository\n",
        "DEST_FILE_PATH = \"distribution_centers.json\"         # Path where JSON will be saved"
      ],
      "metadata": {
        "id": "7nJQI1qY6P2h"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 3 - Lambda Functions For ETL Process**"
      ],
      "metadata": {
        "id": "j0hAHft78MVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create_headers > build_url > decode_content > encode_content > parse_csv > to_json"
      ],
      "metadata": {
        "id": "w4nZQtMPAzyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lambda fx to create GitHub API headers\n",
        "# Purpose: Adds authentication & specifies we're sending JSON\n",
        "create_headers = lambda token: {\n",
        "    \"Authorization\": f\"Bearer {token}\",           # authenticates our request\n",
        "    \"Accept\": \"application/vnd.github.v3+json\",  # specifies GitHub API version\n",
        "    \"Content-Type\": \"application/json\"           # tells GitHub we're sending JSON\n",
        "}\n",
        "\n",
        "# Lambda fx to construct GitHub API URL\n",
        "# Purpose: Builds the correct URL to access files in a repository\n",
        "build_url = lambda owner, repo, path: f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n",
        "\n",
        "# Lambda fx to decode base64 content\n",
        "# Purpose: GitHub returns file content in base64, this decodes it to text\n",
        "decode_content = lambda content: base64.b64decode(content).decode('utf-8')\n",
        "\n",
        "# Lambda fx to encode content to base64\n",
        "# Purpose: GitHub requires file uploads to be base64 encoded\n",
        "encode_content = lambda content: base64.b64encode(content.encode('utf-8')).decode('utf-8')\n",
        "\n",
        "# Lambda fx to parse CSV to list of dictionaries\n",
        "# Purpose: Convert CSV rows into Python dictionaries for easy manipulation\n",
        "parse_csv = lambda csv_text: list(csv.DictReader(StringIO(csv_text)))\n",
        "\n",
        "# Lambda fx to convert list to JSON String\n",
        "# Purpose: Transforms python data structure into formatted JSON\n",
        "to_json = lambda data: json.dumps(data, indent=2)      # indent=2 makes it readable"
      ],
      "metadata": {
        "id": "pS4IZ6rf8Uhw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 4 - Extract Function**"
      ],
      "metadata": {
        "id": "MhrpJf8bBNeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_csv_from_github(owner, repo, file_path, token):\n",
        "  # docstrings are the text in triple quotes \"\"\"...\"\"\" that explain what a function does\n",
        "  # purpose: for professional & helps others (and future you) understand the code!\n",
        "  \"\"\"\n",
        "  Extract CSV data from GitHub repository\n",
        "\n",
        "  Args:\n",
        "      owner: GitHub username/organization\n",
        "      repo: Repository name\n",
        "      file_path: Path to file within repository\n",
        "      token: GitHub personal access token\n",
        "\n",
        "  Returns:\n",
        "      String content of the CSV file\n",
        "  \"\"\"\n",
        "\n",
        "  # build the API URL for the file\n",
        "  url = build_url(owner, repo, file_path)\n",
        "\n",
        "  # create authentication headers\n",
        "  headers = create_headers(token)\n",
        "\n",
        "  # make GET request to GitHub API\n",
        "  print(f\"üì• Extracting data from: {owner}/{repo}/{file_path}\")\n",
        "  response = requests.get(url, headers=headers)\n",
        "\n",
        "  # check if request was successful\n",
        "  if response.status_code == 200:\n",
        "      # parse the JSON response\n",
        "      file_data = response.json()\n",
        "\n",
        "      # decode the base64 content to get actual CSV text\n",
        "      csv_content = decode_content(file_data['content'])\n",
        "\n",
        "      print(\"‚úÖ Extraction successful!\")\n",
        "      return csv_content\n",
        "  else:\n",
        "      # if request failed ,raise an error with details\n",
        "      raise Exception(f\"‚ùå Failed to extract: {response.status_code} - {response.text}\")"
      ],
      "metadata": {
        "id": "NoiOyB0lBSwd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 5 - Transform Function**"
      ],
      "metadata": {
        "id": "lWEheyyMnX3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_csv_to_json(csv_content):\n",
        "  \"\"\"\n",
        "  Transform CSV content to JSON format\n",
        "\n",
        "  Args:\n",
        "      csv_content: String content of CSV file\n",
        "\n",
        "  Returns:\n",
        "      JSON string representation of the data\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"üîÑTransforming SCV to JSON...\")\n",
        "\n",
        "  # parse CSV text into list of dictionaries using lambda fx\n",
        "  data = parse_csv(csv_content)\n",
        "\n",
        "  # convert to JSON string using lambda fx\n",
        "  json_content = to_json(data)\n",
        "\n",
        "  print(f\"‚úÖTransformation complete! Converted {len(data)} rows\")\n",
        "  return json_content"
      ],
      "metadata": {
        "id": "WM--iw-8nes6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 6 - Load Function**"
      ],
      "metadata": {
        "id": "u3ZZFqeQo8bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json_to_github(owner, repo, file_path, json_content, token, commit_message=\"ETL: Upload transformed data\"):\n",
        "  \"\"\"\n",
        "  Load JSON data to GitHub repository\n",
        "\n",
        "  Args:\n",
        "      owner: GitHub username/organization\n",
        "      repo: Repository name\n",
        "      file_path: Path where file should be saved\n",
        "      json_content: JSON string to upload\n",
        "      token: GitHub personal access token\n",
        "      commit_message: Git commit message\n",
        "\n",
        "  Returns:\n",
        "      Response from GitHub API\n",
        "  \"\"\"\n",
        "\n",
        "  print(f\"üì§Loading data to: {owner}/{repo}/{file_path}\")\n",
        "\n",
        "  # build the API URL for destination / target\n",
        "  url = build_url(owner, repo, file_path)\n",
        "\n",
        "  # create authentication headers\n",
        "  headers = create_headers(token)\n",
        "\n",
        "  # first, check if file already exists (to get SHA for update)\n",
        "  check_response = requests.get(url, headers=headers)\n",
        "\n",
        "  # prepare the payload for GitHub API\n",
        "  payload = {\n",
        "      \"message\": commit_message,                # git commit message\n",
        "      \"content\": encode_content(json_content),  # base64 encoded content\n",
        "      \"branch\": \"main\"                          # target branch (change if needed)\n",
        "  }\n",
        "\n",
        "  # if file exists, we need to include its SHA for update\n",
        "  if check_response.status_code == 200:\n",
        "     payload[\"sha\"] = check_response.json()[\"sha\"]\n",
        "     print(\"üìùFile exists, updating...\")\n",
        "  else:\n",
        "     print(\"üìùCreating new file...\")\n",
        "\n",
        "  # make PUT request to create/update file\n",
        "  response = requests.put(url, headers=headers, json=payload)\n",
        "\n",
        "  # check if upload was successful\n",
        "  if response.status_code in [200, 201]:\n",
        "     print(\"‚úÖLoad successful!\")\n",
        "     return response.json()\n",
        "  else:\n",
        "     raise Exception(f\"‚ùå Failed to load: {response.status_code} - {response.text}\")"
      ],
      "metadata": {
        "id": "SHQcRaKTpAr7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 7 - Main ETL Pipeline**"
      ],
      "metadata": {
        "id": "Ya5NWc2wtBJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_etl_pipeline():\n",
        "  \"\"\"\n",
        "  Execute the complete ETL pipeline\n",
        "\n",
        "  This function orchestrates the Extract, Transform, Load process\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"\\n\" + \"=\"*60)\n",
        "  print(\"üöÄ Starting ETL Pipeline: GitHub CSV ‚Üí JSON Transfer\")\n",
        "  print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "  try:\n",
        "      # EXTRACT: Get CSV data from source repository\n",
        "      csv_data = extract_csv_from_github(\n",
        "          owner=SOURCE_OWNER,\n",
        "          repo=SOURCE_REPO,\n",
        "          file_path=SOURCE_FILE_PATH,\n",
        "          token=GITHUB_TOKEN\n",
        "      )\n",
        "\n",
        "      print()  # empty line for readability\n",
        "\n",
        "      # TRANSFORM: Convert CSV to JSON\n",
        "      json_data = transform_csv_to_json(csv_data)\n",
        "\n",
        "      print()  # empty line for readability\n",
        "\n",
        "      # LOAD: Upload JSON to destination repository\n",
        "      result = load_json_to_github(\n",
        "          owner=DEST_OWNER,\n",
        "          repo=DEST_REPO,\n",
        "          file_path=DEST_FILE_PATH,\n",
        "          json_content=json_data,\n",
        "          token=GITHUB_TOKEN,\n",
        "          commit_message=\"ETL Pipeline: Automated CSV to JSON conversion\"\n",
        "      )\n",
        "\n",
        "      print(\"\\n\" + \"=\"*60)\n",
        "      print(\"üéâ ETL Pipeline completed successfully!\")\n",
        "      print(\"=\"*60)\n",
        "      print(f\"\\nüìä File uploaded to: {result['content']['html_url']}\")\n",
        "\n",
        "  except Exception as e:\n",
        "      print(\"\\n\" + \"=\"*60)\n",
        "      print(f\"üí• ETL Pipeline failed: {str(e)}\")\n",
        "      print(\"=\"*60)"
      ],
      "metadata": {
        "id": "GDeH1RSEtGbx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 8 - Execute The Pipeline**"
      ],
      "metadata": {
        "id": "Y6fIeDvJwuc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Run the ETL pipeline\n",
        "    run_etl_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq6MeNmBwzYO",
        "outputId": "59ebae91-7204-448d-fd5d-700b48157669"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üöÄ Starting ETL Pipeline: GitHub CSV ‚Üí JSON Transfer\n",
            "============================================================\n",
            "\n",
            "üì• Extracting data from: Shazizan/data/distribution_centers.csv\n",
            "‚úÖ Extraction successful!\n",
            "\n",
            "üîÑTransforming SCV to JSON...\n",
            "‚úÖTransformation complete! Converted 10 rows\n",
            "\n",
            "üì§Loading data to: Shazizan/pipeline-vault/distribution_centers.json\n",
            "üìùCreating new file...\n",
            "‚úÖLoad successful!\n",
            "\n",
            "============================================================\n",
            "üéâ ETL Pipeline completed successfully!\n",
            "============================================================\n",
            "\n",
            "üìä File uploaded to: https://github.com/Shazizan/pipeline-vault/blob/main/distribution_centers.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional Helper Functions (optional but useful)**"
      ],
      "metadata": {
        "id": "9biwk9zMw3Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lambda to validate GitHub token format\n",
        "validate_token = lambda token: token.startswith(('ghp_', 'github_pat_'))\n",
        "\n",
        "# Lambda to get file extension\n",
        "get_extension = lambda filename: filename.split('.')[-1]\n",
        "\n",
        "# Lambda to create timestamp for unique filenames\n",
        "from datetime import datetime\n",
        "create_timestamp = lambda: datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Example: Create unique output filename with timestamp\n",
        "# unique_filename = lambda base: f\"{base}_{create_timestamp()}.json\""
      ],
      "metadata": {
        "id": "Ip2dDflzxc8F"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}