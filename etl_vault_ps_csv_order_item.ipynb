{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf2SPcYbn2elmpnTYzEr1G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shazizan/portfolio/blob/master/etl_vault_ps_csv_order_item.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Up Spark**"
      ],
      "metadata": {
        "id": "p05kyTA5fIV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install Java & PySpark**"
      ],
      "metadata": {
        "id": "uNmtagBXhtLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: As colab does not come with Java by default, and PySpark needs it."
      ],
      "metadata": {
        "id": "hIxY1EQ9h9SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbkiM5V5gYxK",
        "outputId": "7e450fcf-b465-4595-ed83-26bc50c6a1f9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set JAVA_HOME environment variable**"
      ],
      "metadata": {
        "id": "LgAwxoxsiV4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: This tells PySpark where Java is located."
      ],
      "metadata": {
        "id": "uzsbj_4Zipwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
      ],
      "metadata": {
        "id": "IyqUp8iyiQqU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Verify Installation above (Java & PySpark)**"
      ],
      "metadata": {
        "id": "E4ynBLa1i0jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version\n",
        "!python --version\n",
        "!pip show pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK0ZpKwfixoz",
        "outputId": "15e5c49a-c916-4443-b036-18ec65361ede"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.28\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu122.04.1, mixed mode, sharing)\n",
            "Python 3.12.12\n",
            "Name: pyspark\n",
            "Version: 3.5.1\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: py4j\n",
            "Required-by: dataproc-spark-connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Start Spark Session**"
      ],
      "metadata": {
        "id": "RQQHA_f4jr2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: PySpark’s read.csv() does not support reading from HTTP/HTTPS URLs directly. It only supports:\n",
        "\n",
        "- Local file paths (/content/...)\n",
        "- Distributed file systems (e.g. HDFS, S3, GCS)\n",
        "- Mounted paths (e.g. DBFS in Databricks)\n",
        "\n",
        "Therefore, when we want to pass a raw GitHub URL, Spark can’t handle it — it expects a file system protocol, not an HTTP stream."
      ],
      "metadata": {
        "id": "T9k-4Rhumunc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively: We need to read data using pandas and then convert it into Spark."
      ],
      "metadata": {
        "id": "TkXnmH3_nhQO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "D6uYEqS7eFg1",
        "outputId": "b5406488-9971-4ce7-9cf2-80f3cb50cbf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cdc10dd9a00>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://21183270cb45:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>ETL_by_PySpark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ETL_by_PySpark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extraction / Read data from Source Repo**"
      ],
      "metadata": {
        "id": "V6N8F0wqntQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = pd.read_csv(\"https://raw.githubusercontent.com/Shazizan/data/refs/heads/master/order_items.csv\")\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMollTrjfnmS",
        "outputId": "9a281e4d-605b-4e0f-c623-a5ad0d1144b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-------+----------+-----------------+----------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "|    id|order_id|user_id|product_id|inventory_item_id|    status|          created_at|          shipped_at|        delivered_at|         returned_at|        sale_price|\n",
            "+------+--------+-------+----------+-----------------+----------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "| 97315|   67028|  53520|     14235|           263063| Cancelled|2023-01-03 11:14:...|                 NaN|                 NaN|                 NaN|0.0199999995529651|\n",
            "| 44354|   30556|  24316|     14235|           119879|  Complete|2024-10-19 07:20:...|2024-10-21 01:13:...|2024-10-24 08:27:...|                 NaN|0.0199999995529651|\n",
            "| 43903|   30246|  24063|     14235|           118670|Processing|2025-07-19 07:01:...|                 NaN|                 NaN|                 NaN|0.0199999995529651|\n",
            "| 16788|   11573|   9171|     14235|            45378|  Returned|2025-03-13 10:04:...|2025-03-16 02:29:...|2025-03-20 05:03:...|2025-03-21 14:31:...|0.0199999995529651|\n",
            "|179770|  123870|  99207|     14235|           485637|   Shipped|2022-12-09 07:03:...|2022-12-11 05:37:...|                 NaN|                 NaN|0.0199999995529651|\n",
            "+------+--------+-------+----------+-----------------+----------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformation**"
      ],
      "metadata": {
        "id": "Cjh78X35q4JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, I decided to do data cleaning\n",
        "- check the null value / NaN\n",
        "- Convert NaN into Null"
      ],
      "metadata": {
        "id": "wQMSRWf-rGIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KQ7PWtBoh6T",
        "outputId": "7360cc2f-a22e-4ca0-c5a1-7f9bbc1ccb64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "181248"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- null → Missing value (no data at all)\n",
        "- NaN → Not-a-Number (only for numeric columns, e.g., float or double)"
      ],
      "metadata": {
        "id": "b4m1jpQMqLv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Check for nulls per column**"
      ],
      "metadata": {
        "id": "1fTT1e-Dpz00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum as _sum\n",
        "\n",
        "df.select([\n",
        "    _sum(col(c).isNull().cast(\"int\")).alias(c)\n",
        "    for c in df.columns\n",
        "]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRF9AGpmpDwz",
        "outputId": "ab943da9-ef52-401a-fd7e-491006571108"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+-------+----------+-----------------+------+----------+----------+------------+-----------+----------+\n",
            "| id|order_id|user_id|product_id|inventory_item_id|status|created_at|shipped_at|delivered_at|returned_at|sale_price|\n",
            "+---+--------+-------+----------+-----------------+------+----------+----------+------------+-----------+----------+\n",
            "|  0|       0|      0|         0|                0|     0|         0|         0|           0|          0|         0|\n",
            "+---+--------+-------+----------+-----------------+------+----------+----------+------------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Count NaNs per column**"
      ],
      "metadata": {
        "id": "iDCkxzVdqBG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum as _sum, isnan\n",
        "\n",
        "df.select([\n",
        "    _sum(isnan(col(c)).cast(\"int\")).alias(c)\n",
        "    for c in df.columns\n",
        "]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCvqw3FNpbsB",
        "outputId": "52e9a740-e42f-4a0d-8754-994834fa68b8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+-------+----------+-----------------+------+----------+----------+------------+-----------+----------+\n",
            "| id|order_id|user_id|product_id|inventory_item_id|status|created_at|shipped_at|delivered_at|returned_at|sale_price|\n",
            "+---+--------+-------+----------+-----------------+------+----------+----------+------------+-----------+----------+\n",
            "|  0|       0|      0|         0|                0|     0|         0|     63220|      117309|     162963|         0|\n",
            "+---+--------+-------+----------+-----------------+------+----------+----------+------------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Observation**"
      ],
      "metadata": {
        "id": "_5xXgFquqmJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Since this is an order tracking data:\n",
        "\n",
        "- the columns - shipped_at, delivered_at, returned_at → some NaNs are expected (e.g., cancelled orders)\n",
        "- We might not want to drop these rows completely. Instead, consider filling them with null and the Data Analyst can handle them during analysis."
      ],
      "metadata": {
        "id": "xxPoMJg_vVK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Convert the NaN value into Null**"
      ],
      "metadata": {
        "id": "nmPC4YNJw26d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "columns_to_check = [\"shipped_at\", \"delivered_at\", \"returned_at\"]\n",
        "\n",
        "for c in columns_to_check:\n",
        "    df = df.withColumn(c, when(col(c) == \"NaN\", None).otherwise(col(c)))"
      ],
      "metadata": {
        "id": "ZZ0xzk2kw-N4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show top 5 rows\n",
        "df.show(5)\n",
        "\n",
        "# Check schema and nulls\n",
        "df.printSchema()\n",
        "\n",
        "# Count total rows\n",
        "df.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_neokE3vxhz8",
        "outputId": "9015f184-f148-4309-cb9f-f4cc0e898292"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+-------+----------+-----------------+----------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "|    id|order_id|user_id|product_id|inventory_item_id|    status|          created_at|          shipped_at|        delivered_at|         returned_at|        sale_price|\n",
            "+------+--------+-------+----------+-----------------+----------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "| 97315|   67028|  53520|     14235|           263063| Cancelled|2023-01-03 11:14:...|                NULL|                NULL|                NULL|0.0199999995529651|\n",
            "| 44354|   30556|  24316|     14235|           119879|  Complete|2024-10-19 07:20:...|2024-10-21 01:13:...|2024-10-24 08:27:...|                NULL|0.0199999995529651|\n",
            "| 43903|   30246|  24063|     14235|           118670|Processing|2025-07-19 07:01:...|                NULL|                NULL|                NULL|0.0199999995529651|\n",
            "| 16788|   11573|   9171|     14235|            45378|  Returned|2025-03-13 10:04:...|2025-03-16 02:29:...|2025-03-20 05:03:...|2025-03-21 14:31:...|0.0199999995529651|\n",
            "|179770|  123870|  99207|     14235|           485637|   Shipped|2022-12-09 07:03:...|2022-12-11 05:37:...|                NULL|                NULL|0.0199999995529651|\n",
            "+------+--------+-------+----------+-----------------+----------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- order_id: long (nullable = true)\n",
            " |-- user_id: long (nullable = true)\n",
            " |-- product_id: long (nullable = true)\n",
            " |-- inventory_item_id: long (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- created_at: string (nullable = true)\n",
            " |-- shipped_at: string (nullable = true)\n",
            " |-- delivered_at: string (nullable = true)\n",
            " |-- returned_at: string (nullable = true)\n",
            " |-- sale_price: double (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "181248"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Save the transformed data into Parquet**"
      ],
      "metadata": {
        "id": "KA7ucs1Ly-bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as Parquet locally\n",
        "df.write.parquet(\"cleaned_orders_item\", mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "q0mPMEdmytoL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load**"
      ],
      "metadata": {
        "id": "wBm07WNvzFYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the parquet file into the gituhub repo (target system) using Github API"
      ],
      "metadata": {
        "id": "VGsAG7Vg9YtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install PyGitHub**"
      ],
      "metadata": {
        "id": "HL-eTui-zVML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Only installs the library. Without it, we can’t use the GitHub API."
      ],
      "metadata": {
        "id": "oGUAPkDPAlw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyGithub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH4qh0eB8v0k",
        "outputId": "ba075c1f-61e5-47c1-ddcc-a371edd00f3d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (1.6.0)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (2.32.4)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (2.5.0)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from pynacl>=1.4.0->PyGithub) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (2025.10.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import and Authenticate**"
      ],
      "metadata": {
        "id": "hQ7xOu039H1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: Lets Python “log in” to GitHub using our token. Without authentication, we cannot push files to the repo."
      ],
      "metadata": {
        "id": "q26wDPBYAHmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from github import Github, Auth\n",
        "\n",
        "# Create authentication using Auth\n",
        "auth = Auth.Token(\"YOUR_GITHUB_TOKEN\")\n",
        "\n",
        "# Pass auth object to Github\n",
        "g = Github(auth=auth)\n",
        "\n",
        "# Access your repo\n",
        "repo = g.get_user().get_repo(\"pipeline-vault\")"
      ],
      "metadata": {
        "id": "pmLr5p4D87DA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Upload or update a Parquet file**"
      ],
      "metadata": {
        "id": "ferkj5Ji_t1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight: This is the actual step that sends our Parquet file to GitHub."
      ],
      "metadata": {
        "id": "E5AfAkiT_w19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "\n",
        "# Local Parquet file in Colab\n",
        "local_file_path = \"cleaned_orders_item.parquet\"\n",
        "\n",
        "# Path in the GitHub repo\n",
        "repo_file_path = \"cleaned_orders_item.parquet\"\n",
        "\n",
        "# Read the file as bytes\n",
        "with open(local_file_path, \"rb\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Upload the file: create if not exists, otherwise update\n",
        "try:\n",
        "    # Check if file exists\n",
        "    existing_file = repo.get_contents(repo_file_path)\n",
        "    # Update existing file\n",
        "    repo.update_file(\n",
        "        path=existing_file.path,\n",
        "        message=\"Update cleaned orders Parquet\",\n",
        "        content=content,\n",
        "        sha=existing_file.sha\n",
        "    )\n",
        "    print(\"File updated successfully!\")\n",
        "except:\n",
        "    # File does not exist → create it\n",
        "    repo.create_file(\n",
        "        path=repo_file_path,\n",
        "        message=\"Add cleaned orders Parquet\",\n",
        "        content=content\n",
        "    )\n",
        "    print(\"File uploaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dARaCQ9Q_Tfa",
        "outputId": "00225d56-7a34-4461-cf2c-0d03582463a0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File uploaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Verify the File into Target Repo**"
      ],
      "metadata": {
        "id": "3FeU7D0zASDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the file should appear in the repo: pipeline-vault"
      ],
      "metadata": {
        "id": "7PoYbfM7AZlQ"
      }
    }
  ]
}