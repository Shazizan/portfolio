{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP1fdlrFUHtncw6LxzTrnsE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shazizan/portfolio/blob/master/etl_vault_ps_realtime_openweathermap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Real-Time Weather Data ETL Pipeline Using PySpark and OpenWeather API**"
      ],
      "metadata": {
        "id": "walW2PCXm0xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting Data Ready**"
      ],
      "metadata": {
        "id": "8mEWlLZanhWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gonna fetch the real-time data of weather here: https://home.openweathermap.org/api_keys\n",
        ".\n",
        "- get the key"
      ],
      "metadata": {
        "id": "iXuvLMAWn5zI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test The Key / Fetch the data using API key**"
      ],
      "metadata": {
        "id": "jEWys5inoUZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output data will display in the json format."
      ],
      "metadata": {
        "id": "YtQ5cuwEywNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_KEY = \"2c610b16a354e9d1a696c6a905f96321\"\n",
        "CITY = \"Kuala Lumpur\"\n",
        "URL = f\"http://api.openweathermap.org/data/2.5/weather?q={CITY}&appid={API_KEY}&units=metric\"\n",
        "\n",
        "response = requests.get(URL).json()\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuWbe5RYmrz6",
        "outputId": "25dd4704-8200-416a-b027-d342299322ec"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'coord': {'lon': 101.6865, 'lat': 3.1431}, 'weather': [{'id': 801, 'main': 'Clouds', 'description': 'few clouds', 'icon': '02n'}], 'base': 'stations', 'main': {'temp': 26.75, 'feels_like': 29.94, 'temp_min': 26.54, 'temp_max': 26.75, 'pressure': 1011, 'humidity': 89, 'sea_level': 1011, 'grnd_level': 998}, 'visibility': 9000, 'wind': {'speed': 0.51, 'deg': 0}, 'clouds': {'all': 20}, 'dt': 1759411432, 'sys': {'type': 1, 'id': 9446, 'country': 'MY', 'sunrise': 1759359601, 'sunset': 1759403108}, 'timezone': 28800, 'id': 1733046, 'name': 'Kuala Lumpur', 'cod': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7pah1NnP5pn",
        "outputId": "fc71f4ca-55a5-4be7-db35-b2784eabbc21"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'coord': {'lon': 101.6865, 'lat': 3.1431},\n",
              " 'weather': [{'id': 801,\n",
              "   'main': 'Clouds',\n",
              "   'description': 'few clouds',\n",
              "   'icon': '02n'}],\n",
              " 'base': 'stations',\n",
              " 'main': {'temp': 26.75,\n",
              "  'feels_like': 29.94,\n",
              "  'temp_min': 26.54,\n",
              "  'temp_max': 26.75,\n",
              "  'pressure': 1011,\n",
              "  'humidity': 89,\n",
              "  'sea_level': 1011,\n",
              "  'grnd_level': 998},\n",
              " 'visibility': 9000,\n",
              " 'wind': {'speed': 0.51, 'deg': 0},\n",
              " 'clouds': {'all': 20},\n",
              " 'dt': 1759411432,\n",
              " 'sys': {'type': 1,\n",
              "  'id': 9446,\n",
              "  'country': 'MY',\n",
              "  'sunrise': 1759359601,\n",
              "  'sunset': 1759403108},\n",
              " 'timezone': 28800,\n",
              " 'id': 1733046,\n",
              " 'name': 'Kuala Lumpur',\n",
              " 'cod': 200}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Up Spark Configuration**"
      ],
      "metadata": {
        "id": "ueKx9-0_yHSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install Required Libraries**"
      ],
      "metadata": {
        "id": "wt0JF2umxmLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6gXJlQ8xk5g",
        "outputId": "78063fc1-79d3-45e7-933a-1d3527b485cb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "start spark session - Engine for spark"
      ],
      "metadata": {
        "id": "4FzLAPK0ztrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"WeatherStreamETL\").getOrCreate()"
      ],
      "metadata": {
        "id": "AqEuaX_0yQMU"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define the Schema**"
      ],
      "metadata": {
        "id": "i6yB3GqWz-fV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- A schema is like a blueprint for the data. Example, \"What kind of data I‚Äôm giving you\" ‚Äî \"what each column is called and what type of value it has.‚Äù\n",
        "- Why we need this? Because PySpark is built to handle massive data (like millions of rows). So it needs to know ahead of time, What are the column names & What data type each column is (number, text, etc.)"
      ],
      "metadata": {
        "id": "gf7eqhDgC-82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tells Spark:\n",
        "- id ‚Üí whole number (Integer)\n",
        "- value ‚Üí whole number (Integer)\n",
        "- category ‚Üí text (String)\n",
        "- True ‚Üí means ‚Äúthis column can have empty (null) values‚Äù"
      ],
      "metadata": {
        "id": "lFBoWJ_QD4to"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps Spark:\n",
        "- üèéÔ∏è Process faster (it knows how to handle each column properly)\n",
        "- üö´ Avoid confusion (e.g., not mix numbers with text)\n",
        "- ‚úÖ Validate data (catch errors if a value doesn‚Äôt match the type)"
      ],
      "metadata": {
        "id": "dHtUh_yeGZbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"temperature\", DoubleType(), True),\n",
        "    StructField(\"feels_like\", DoubleType(), True),\n",
        "    StructField(\"humidity\", IntegerType(), True),\n",
        "    StructField(\"weather\", StringType(), True),\n",
        "    StructField(\"description\", StringType(), True),\n",
        "    StructField(\"wind_speed\", DoubleType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "SY-unoShLcKd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract / Fetch Data from API**"
      ],
      "metadata": {
        "id": "qPRs9ACAFfVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "API_KEY = \"2c610b16a354e9d1a696c6a905f96321\"  # replace with my key\n",
        "CITY = \"Kuala Lumpur\"\n",
        "URL = f\"http://api.openweathermap.org/data/2.5/weather?q={CITY}&appid={API_KEY}&units=metric\"\n",
        "\n",
        "def get_weather():\n",
        "    response = requests.get(URL).json()\n",
        "    data = {\n",
        "        \"city\": [response.get(\"name\", \"\")],\n",
        "        \"country\": [response.get(\"sys\", {}).get(\"country\", \"\")],\n",
        "        \"temperature\": [response.get(\"main\", {}).get(\"temp\", None)],\n",
        "        \"feels_like\": [response.get(\"main\", {}).get(\"feels_like\", None)],\n",
        "        \"humidity\": [response.get(\"main\", {}).get(\"humidity\", None)],\n",
        "        \"weather\": [response.get(\"weather\", [{}])[0].get(\"main\", \"\")],\n",
        "        \"description\": [response.get(\"weather\", [{}])[0].get(\"description\", \"\")],\n",
        "        \"wind_speed\": [response.get(\"wind\", {}).get(\"speed\", None)]\n",
        "    }\n",
        "    return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "rqYW7bq5EnqR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- units=metric gives temperature in Celsius.\n",
        "- The function returns a Pandas DataFrame ready to convert to PySpark."
      ],
      "metadata": {
        "id": "THZEsNTbE9nE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stream Data in Micro-Batches + Apply Transformation inside Batch**"
      ],
      "metadata": {
        "id": "wjP3mckSF84R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulate streaming by fetching data every few seconds:"
      ],
      "metadata": {
        "id": "cr2LKot7GwfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "import time\n",
        "\n",
        "for batch_num in range(5):  # 5 micro-batches\n",
        "    print(f\"Batch {batch_num+1}\")\n",
        "    pdf = get_weather()\n",
        "    df = spark.createDataFrame(pdf, schema=schema)\n",
        "\n",
        "    # transformation: only show temperatures below 30¬∞C\n",
        "    transformed = df.filter(col(\"temperature\") < 30)\n",
        "    transformed.show()\n",
        "\n",
        "    time.sleep(5)  # wait 5 seconds to simulate streaming\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIcAruL0Fata",
        "outputId": "b2b517a2-f21b-4c41-aba9-45f6f4fbacfc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "|        city|country|temperature|feels_like|humidity|weather|description|wind_speed|\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "|Kuala Lumpur|     MY|      26.75|     29.94|      89| Clouds| few clouds|      0.51|\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "\n",
            "Batch 2\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "|        city|country|temperature|feels_like|humidity|weather|description|wind_speed|\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "|Kuala Lumpur|     MY|      26.75|     29.94|      89| Clouds| few clouds|      0.51|\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "\n",
            "Batch 3\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "|        city|country|temperature|feels_like|humidity|weather|description|wind_speed|\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "|Kuala Lumpur|     MY|      26.75|     29.94|      89| Clouds| few clouds|      0.51|\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "\n",
            "Batch 4\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "|        city|country|temperature|feels_like|humidity|weather|description|wind_speed|\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "|Kuala Lumpur|     MY|      26.75|     29.94|      89| Clouds| few clouds|      0.51|\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "\n",
            "Batch 5\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "|        city|country|temperature|feels_like|humidity|weather|description|wind_speed|\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "|Kuala Lumpur|     MY|      26.75|     29.94|      89| Clouds| few clouds|      0.51|\n",
            "+------------+-------+-----------+----------+--------+-------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- Here I'mfetching the data repeatedly in small batches (micro-batches)\n",
        "- Each batch is treated like a ‚Äúmini-stream‚Äù of data.\n",
        "- The time.sleep(5) simulates real-time streaming."
      ],
      "metadata": {
        "id": "C3U01NEANjCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where does micro-batching fit?\n",
        "- Micro-batching: just the mechanism to feed data in chunks ‚Üí this is part of the streaming/ETL workflow, not the transformation itself.\n",
        "\n",
        "- Transformation: happens inside each batch, when filter, select, or manipulate the data."
      ],
      "metadata": {
        "id": "GatsCzCAPOjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load**"
      ],
      "metadata": {
        "id": "dNmmi1_oHIFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I want to load the processed data weather data into the target (GitHub) repo"
      ],
      "metadata": {
        "id": "J5Z0e0G7IKU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1 - Install PyGithub**"
      ],
      "metadata": {
        "id": "bFgYuQAtIf5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyGithub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WslOztPKImRJ",
        "outputId": "598ef781-b99a-4df3-ab2a-275474e9fe5d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyGithub\n",
            "  Downloading pygithub-2.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pynacl>=1.4.0 (from PyGithub)\n",
            "  Downloading pynacl-1.6.0-cp38-abi3-manylinux_2_34_x86_64.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (2.32.4)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (2.5.0)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from pynacl>=1.4.0->PyGithub) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (2025.8.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.23)\n",
            "Downloading pygithub-2.8.1-py3-none-any.whl (432 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m432.7/432.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynacl-1.6.0-cp38-abi3-manylinux_2_34_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynacl, PyGithub\n",
            "Successfully installed PyGithub-2.8.1 pynacl-1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2 - Generate a GitHub Personal Access Token (PAT)**"
      ],
      "metadata": {
        "id": "6M3mO-z6IwHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In Github, I'm gonna generate the personal token access.\n",
        "- However, this token is confidential, which I must delete it before I can save this job."
      ],
      "metadata": {
        "id": "DlOANaKuI0JC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3 - Import and Authenticate**"
      ],
      "metadata": {
        "id": "O7exJplBJrwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from github import Github, Auth\n",
        "\n",
        "# Create authentication using Auth\n",
        "auth = Auth.Token(\"REPLACE_WITH_YOUR_GITHUB_TOKEN\")\n",
        "\n",
        "# Pass auth object to Github\n",
        "g = Github(auth=auth)\n",
        "\n",
        "# Access your repo\n",
        "repo = g.get_user().get_repo(\"pipeline-vault\")\n"
      ],
      "metadata": {
        "id": "27YP3ydCLJrM"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- The Auth.Token(\"XXXXX\") line uses my Personal Access Token (PAT) to log in to GitHub.\n",
        "- This tells GitHub: ‚ÄúI‚Äôm Shazizan, and I allow this notebook to act on my behalf.‚Äù\n",
        "- repo = g.get_user().get_repo(\"pipeline-vault\") - This connects to my repo named & now can interact with that repo programmatically\n",
        "- üëâ Nothing is uploaded yet ‚Äî it‚Äôs just like signing in.\n"
      ],
      "metadata": {
        "id": "guKRA3bZUR7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4 - Create a temporary folder & Convert PySpark DataFrame to CSV**"
      ],
      "metadata": {
        "id": "tmtwsuQuMgbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GitHub cannot accept a PySpark DataFrame directly ‚Äî it only accepts files (CSV, JSON, etc.)\n",
        "- This step creates a real CSV file that you can upload programmatically."
      ],
      "metadata": {
        "id": "iO9PQx7FMp4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Make a temporary folder\n",
        "os.makedirs(\"temp_data\", exist_ok=True)\n",
        "\n",
        "# Save as CSV locally (one file for simplicity)\n",
        "transformed.toPandas().to_csv(\"temp_data/weather_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "wprBJPPrMg5m"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- os.makedirs(\"temp_data\") ‚Üí creates a folder named temp_data in your Colab environment.\n",
        "- exist_ok=True ‚Üí if the folder already exists, don‚Äôt throw an error.\n",
        "- ‚úÖ Purpose: to have a place to save your CSV file temporarily before uploading to GitHub.\n",
        "- transformed ‚Üí your PySpark DataFrame (the output after your ‚Äútransformation‚Äù step).\n",
        "- .toPandas() ‚Üí converts the PySpark DataFrame into a Pandas DataFrame (because PySpark cannot directly write files in this simple way).\n",
        "- .to_csv(\"temp_data/weather_data.csv\", index=False) ‚Üí saves the Pandas DataFrame as a CSV file inside the folder temp_data."
      ],
      "metadata": {
        "id": "hSNj5r-ZO5zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5 - Push the CSV to GitHub**"
      ],
      "metadata": {
        "id": "T1k7vEIXTuuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV content\n",
        "with open(\"temp_data/weather_data.csv\", \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "# File path inside repo\n",
        "github_path = \"weather_data.csv\"  # this will appear in the repo root\n",
        "\n",
        "# Check if file exists in repo\n",
        "try:\n",
        "    existing_file = repo.get_contents(github_path)\n",
        "    # Update the file if it exists\n",
        "    repo.update_file(existing_file.path, \"Update weather data\", content, existing_file.sha)\n",
        "    print(\"‚úÖ File updated in GitHub repo\")\n",
        "except:\n",
        "    # Create a new file if it doesn't exist\n",
        "    repo.create_file(github_path, \"Add weather data\", content)\n",
        "    print(\"‚úÖ File created in GitHub repo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6LcAy6-NMY7",
        "outputId": "9da1f09a-a7d9-4984-b7d8-727eebcbd1f3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ File created in GitHub repo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "- The CSV in temp_data is like a staging area.\n",
        "- The GitHub code here transfers that staged CSV into my target repo ‚Äî effectively ‚Äúpushing‚Äù my transformed ETL output.\n",
        "- GitHub itself does not see or know about temp_data ‚Äî it only sees the file after it is uploaded/committed."
      ],
      "metadata": {
        "id": "6fDT8NWzZUQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ **Key point to remember for this ETL experiment:**\n",
        "\n",
        "- **Extract:** `pdf = get_weather()` ‚Üí fetching raw API data  \n",
        "- **Transform:** `transformed = df.filter(col(\"temperature\") < 30)` ‚Üí filtering rows  \n",
        "- **Load:** saving or pushing `transformed` to GitHub or CSV  "
      ],
      "metadata": {
        "id": "zsmtOjW_ORMV"
      }
    }
  ]
}